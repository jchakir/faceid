{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, utils"
      ],
      "metadata": {
        "id": "omC8I-mn3nTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define global varibles as needed"
      ],
      "metadata": {
        "id": "j4MOV27CnPOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "DATASET_DIR = 'dataset'\n",
        "BATCH_SIZE = 10\n",
        "DATASET_SIZE = 5000\n",
        "ID_FROM = 100_000\n",
        "ID_TO = 133_332\n",
        "IMAGES_PEER_ID = 5\n",
        "# IMAGE_SHAPE = (224, 224, 3)\n",
        "IMAGE_SHAPE = (112, 112, 3)\n"
      ],
      "metadata": {
        "id": "H5o7mfgXHbbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Its recomended to use other dataset more quality images than [DigiFace1M](https://github.com/microsoft/DigiFace1M) dataset"
      ],
      "metadata": {
        "id": "Toog-_zbnZef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download first part of [DigiFace1M](https://github.com/microsoft/DigiFace1M) dataset."
      ],
      "metadata": {
        "id": "zq49UW0QmiKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! wget https://facesyntheticspubwedata.blob.core.windows.net/wacv-2023/subjects_100000-133332_5_imgs.zip -o faces.zip\n",
        "# ! mkdir dataset && cd dataset && unzip ../faces.zip && rm -rf ../faces.zip"
      ],
      "metadata": {
        "id": "lXNdI6y5SmeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare DATASET: Set (anchor, positive), (anchor, negative), and labels 1: positive, 0: negative"
      ],
      "metadata": {
        "id": "cbqSbnMnntWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_anchor = []\n",
        "X_verify = []\n",
        "\n",
        "y_dataset: list[int] = []\n",
        "\n",
        "np.random.seed(int(time.time()))\n",
        "\n",
        "def get_rand_id() -> int :\n",
        "  return np.random.randint(ID_FROM, ID_TO)\n",
        "\n",
        "def get_rand_img() -> int :\n",
        "  return np.random.randint(0, IMAGES_PEER_ID)\n",
        "\n",
        "def load_rand_image_of_id(dir: int) -> np.array:\n",
        "  # rand_img: int = get_rand_img()\n",
        "  # path = os.path.join(DATASET_DIR, f'{dir}', f'{rand_img}.png')\n",
        "  path = os.path.join(DATASET_DIR, f'{dir}', '0.png')\n",
        "  # image = utils.load_img(path, color_mode='grayscale')\n",
        "  image = utils.load_img(path)\n",
        "  image = utils.img_to_array(image)\n",
        "  image = utils.normalize(image)\n",
        "  return image\n",
        "\n",
        "def set_dataset() -> None:\n",
        "  global X_dataset, y_dataset\n",
        "\n",
        "  for i in range(DATASET_SIZE // 2):\n",
        "    anchor_id = get_rand_id()\n",
        "\n",
        "    anchor_img = load_rand_image_of_id(anchor_id)\n",
        "    # pos_img = load_rand_image_of_id(anchor_id)\n",
        "    negative_img = load_rand_image_of_id(get_rand_id())\n",
        "\n",
        "    X_anchor.append(anchor_img)\n",
        "    X_verify.append(anchor_img)\n",
        "    X_anchor.append(anchor_img)\n",
        "    X_verify.append(negative_img)\n",
        "\n",
        "    y_dataset.append(1)\n",
        "    y_dataset.append(0)\n",
        "\n",
        "set_dataset()\n",
        "\n",
        "X_anchor = np.asarray(X_anchor, dtype=np.float32)\n",
        "X_verify = np.asarray(X_verify, dtype=np.float32)\n",
        "y_dataset = np.asarray(y_dataset, dtype=np.float32)\n",
        "\n",
        "train_size = (DATASET_SIZE * 4) // 5\n",
        "X_anchor_train, X_anchor_test = X_anchor[:train_size, :, :, :], X_anchor[train_size:, :, :, :]\n",
        "X_verify_train, X_verify_test = X_verify[:train_size, :, :, :], X_verify[train_size:, :, :, :]\n",
        "y_train, y_test = y_dataset[:train_size], y_dataset[train_size:]\n"
      ],
      "metadata": {
        "id": "uQOZg-q3K3RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model and training process"
      ],
      "metadata": {
        "id": "BZLwuO1enmr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###define the input convolutional network"
      ],
      "metadata": {
        "id": "CcVSJQ1NoKT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embeding() -> models.Model:\n",
        "\n",
        "  input = layers.Input(shape=IMAGE_SHAPE, batch_size=BATCH_SIZE, name='input_image')\n",
        "\n",
        "  layer = layers.Conv2D(filters=64, kernel_size=5, strides=(1, 1), padding='same', activation='relu', name='conv1')(input)\n",
        "  layer = layers.MaxPooling2D(pool_size=(2, 2), name='maxpooling1')(layer)\n",
        "  layer = layers.BatchNormalization(name='batchnorm1')(layer)\n",
        "\n",
        "  layer = layers.Conv2D(filters=64, kernel_size=1, strides=(1, 1), padding='same', activation='relu', name='conv2a')(layer)\n",
        "  layer = layers.Conv2D(filters=96, kernel_size=3, strides=(1, 1), padding='same', activation='relu', name='conv2b')(layer)\n",
        "  layer = layers.MaxPooling2D(pool_size=(2, 2), name='maxpooling2')(layer)\n",
        "  layer = layers.BatchNormalization(name='batchnorm2')(layer)\n",
        "\n",
        "  layer = layers.Conv2D(filters=96, kernel_size=1, strides=(1, 1), padding='same', activation='relu', name='conv3a')(layer)\n",
        "  layer = layers.Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same', activation='relu', name='conv3b')(layer)\n",
        "  layer = layers.MaxPooling2D(pool_size=(2, 2), name='maxpooling3')(layer)\n",
        "\n",
        "  layer = layers.Conv2D(filters=128, kernel_size=1, strides=(1, 1), padding='same', activation='relu', name='conv4a')(layer)\n",
        "  layer = layers.Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same', activation='relu', name='conv4b')(layer)\n",
        "\n",
        "  layer = layers.Conv2D(filters=256, kernel_size=1, strides=(1, 1), padding='same', activation='relu', name='conv5a')(layer)\n",
        "  layer = layers.Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same', activation='relu', name='conv5b')(layer)\n",
        "\n",
        "  layer = layers.Conv2D(filters=128, kernel_size=1, strides=(1, 1), padding='same', activation='relu', name='conv6a')(layer)\n",
        "  layer = layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='same', activation='relu', name='conv6b')(layer)\n",
        "  layer = layers.MaxPooling2D(pool_size=(2, 2), name='maxpooling6')(layer)\n",
        "\n",
        "  layer = layers.Conv2D(filters=64, kernel_size=1, strides=(1, 1), padding='same', activation='relu', name='conv7a')(layer)\n",
        "  layer = layers.Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='same', activation='relu', name='conv7b')(layer)\n",
        "  layer = layers.MaxPooling2D(pool_size=(2, 2), name='maxpooling7')(layer)\n",
        "\n",
        "  layer = layers.Flatten(name='flatten')(layer)\n",
        "  output = layers.Dense(units=1024, activation='relu', name='dense')(layer)\n",
        "\n",
        "  return models.Model(inputs=input, outputs=output, name='embeding_model')\n"
      ],
      "metadata": {
        "id": "Hw8TOFzRnMIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distance layer\n",
        "Distance = layers.Lambda(lambda pair: tf.abs(pair[0] - pair[1]), name='distance')"
      ],
      "metadata": {
        "id": "e11fHEjjIrd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_siamese_model() -> models.Model:\n",
        "\n",
        "  anchor_img = layers.Input(shape=IMAGE_SHAPE, batch_size=BATCH_SIZE, name='anchor_img')\n",
        "  verification_img = layers.Input(shape=IMAGE_SHAPE, batch_size=BATCH_SIZE, name='verification_img')\n",
        "\n",
        "  embeding = make_embeding()\n",
        "\n",
        "  anchor_embeding, verification_embeding = embeding(anchor_img), embeding(verification_img)\n",
        "\n",
        "  distance = Distance([anchor_embeding, verification_embeding])\n",
        "\n",
        "  dense = layers.Dense(units=128, activation='relu', name='dense_distance')(distance)\n",
        "\n",
        "  output = layers.Dense(units=1, activation='sigmoid', name='classifier')(dense)\n",
        "\n",
        "  return models.Model(inputs=[anchor_img, verification_img], outputs=output, name='faceid_model')\n"
      ],
      "metadata": {
        "id": "QShDRVjxM5V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_siamese_model()"
      ],
      "metadata": {
        "id": "Rd7uVhQGUvsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # model.summary(expand_nested=True, positions=[0.4, 0.65, 0.75, 1.])\n",
        "# utils.plot_model(\n",
        "#       model,\n",
        "#       to_file=\"faceid_model.png\",\n",
        "#       show_shapes=True,\n",
        "#       show_layer_names=True,\n",
        "#       # rankdir=\"LR\",\n",
        "#       expand_nested=True,\n",
        "#       show_layer_activations=True\n",
        "# )\n"
      ],
      "metadata": {
        "id": "MEF6-p6SIy2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "8W6T2poLcG4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=[X_anchor_train, X_verify_train],y=y_train, batch_size=BATCH_SIZE, epochs=10)"
      ],
      "metadata": {
        "id": "wfn2Y3WOUOK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x=[X_anchor_test, X_verify_test], y=y_test, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "qqbsNzEhcUXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('saved_model.pb')"
      ],
      "metadata": {
        "id": "6PSDmH_KtNNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mwh9X_WkmET9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}